{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Importing everything\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "\n",
        "from astropy.coordinates import SkyCoord\n",
        "import astropy.units as u\n",
        "from astroquery.sdss import SDSS\n",
        "\n",
        "print(\"Astropy + Astroquery loaded successfully ✅\")\n"
      ],
      "metadata": {
        "id": "nuplrKxVvkdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Query SDSS stellar spectra\n",
        "\n",
        "from astroquery.sdss import SDSS\n",
        "from astropy.coordinates import SkyCoord\n",
        "import astropy.units as u\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SDSS.clear_cache()\n",
        "print(\"SDSS cache cleared.\")\n",
        "\n",
        "coords = SkyCoord(ra=2.023*u.degree, dec=14.84*u.degree, frame='icrs')\n",
        "search_radius = 3 * u.arcmin\n",
        "\n",
        "print(\"Querying SDSS for spectra...\")\n",
        "matches = SDSS.query_region(coords, radius=search_radius, spectro=True)\n",
        "\n",
        "if matches is None:\n",
        "    raise RuntimeError(\"SDSS returned None (server-side issue).\")\n",
        "\n",
        "print(f\"Found {len(matches)} objects with spectra.\")\n",
        "\n",
        "matches = matches[:10]\n",
        "spectra_hdus = SDSS.get_spectra(matches=matches)\n",
        "\n",
        "print(f\"Downloaded {len(spectra_hdus)} spectra.\")\n"
      ],
      "metadata": {
        "id": "F0bFrHmX8hJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting wavelength, flux, and metadata\n",
        "\n",
        "data_list = []\n",
        "\n",
        "for i, hdu_list in enumerate(spectra_hdus):\n",
        "    try:\n",
        "        header = hdu_list[0].header\n",
        "        data = hdu_list[1].data\n",
        "\n",
        "        #Converting log wavelength to wavelength (Angstrom)\n",
        "        wavelength = 10 ** data['loglam']\n",
        "        flux = data['flux']\n",
        "\n",
        "        main_class = header.get('CLASS', 'Unknown')\n",
        "        subclass = header.get('SUBCLASS', 'Unknown')\n",
        "        if subclass is None or subclass.strip() == '':\n",
        "            subclass = 'Unknown'\n",
        "\n",
        "        data_list.append({\n",
        "            'index': i,\n",
        "            'class': main_class,\n",
        "            'subclass': subclass,\n",
        "            'wavelength': wavelength,\n",
        "            'flux': flux,\n",
        "            'num_points': len(wavelength)\n",
        "        })\n",
        "\n",
        "        print(f\"Spectrum {i+1}: CLASS={main_class}, SUBCLASS={subclass}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Skipped spectrum {i+1}: {e}\")\n",
        "\n",
        "#Creating DataFrame\n",
        "df_stars = pd.DataFrame(data_list)\n",
        "\n",
        "print(\"\\nSummary table:\")\n",
        "df_stars[['index', 'class', 'subclass', 'num_points']]\n"
      ],
      "metadata": {
        "id": "m1bIM9dA8ujN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ploting the first spectrum (sanity check is important)\n",
        "\n",
        "first = df_stars.iloc[0]\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(first['wavelength'], first['flux'], lw=1)\n",
        "plt.xlabel(\"Wavelength (Å)\")\n",
        "plt.ylabel(\"Flux\")\n",
        "plt.title(\"Raw SDSS Stellar Spectrum\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "P6W4L53386xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a common wavelength grid is a must\n",
        "\n",
        "# Inspecting the wavelength coverages\n",
        "min_waves = []\n",
        "max_waves = []\n",
        "\n",
        "for _, row in df_stars.iterrows():\n",
        "    min_waves.append(row['wavelength'].min())\n",
        "    max_waves.append(row['wavelength'].max())\n",
        "\n",
        "global_min = np.max(min_waves)   # conservative overlap\n",
        "global_max = np.min(max_waves)\n",
        "\n",
        "print(f\"Common wavelength range: {global_min:.1f} Å to {global_max:.1f} Å\")\n",
        "\n",
        "NUM_POINTS = 2000  # ML-friendly size\n",
        "common_wavelength = np.linspace(global_min, global_max, NUM_POINTS)\n",
        "\n",
        "print(\"Common wavelength grid shape:\", common_wavelength.shape)\n"
      ],
      "metadata": {
        "id": "NfHykDuC9QGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Interpolated all spectra onto common grids\n",
        "\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "interpolated_fluxes = []\n",
        "\n",
        "for i, row in df_stars.iterrows():\n",
        "    wave = row['wavelength']\n",
        "    flux = row['flux']\n",
        "\n",
        "\n",
        "    interp_func = interp1d(\n",
        "        wave,\n",
        "        flux,\n",
        "        kind='linear',\n",
        "        bounds_error=False,\n",
        "        fill_value=\"extrapolate\"\n",
        "    )\n",
        "\n",
        "    new_flux = interp_func(common_wavelength)\n",
        "    interpolated_fluxes.append(new_flux)\n",
        "\n",
        "interpolated_fluxes = np.array(interpolated_fluxes)\n",
        "\n",
        "print(\"Interpolated flux array shape:\", interpolated_fluxes.shape)\n"
      ],
      "metadata": {
        "id": "RsHxAlTb9o0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalized each spectrum (z-score normalization)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "normalized_fluxes = []\n",
        "\n",
        "for i in range(interpolated_fluxes.shape[0]):\n",
        "    spectrum = interpolated_fluxes[i].reshape(-1, 1)\n",
        "    norm_spectrum = scaler.fit_transform(spectrum).flatten()\n",
        "    normalized_fluxes.append(norm_spectrum)\n",
        "\n",
        "normalized_fluxes = np.array(normalized_fluxes)\n",
        "\n",
        "print(\"Normalized flux array shape:\", normalized_fluxes.shape)\n",
        "print(\"Mean (first spectrum):\", np.mean(normalized_fluxes[0]))\n",
        "print(\"Std (first spectrum):\", np.std(normalized_fluxes[0]))\n"
      ],
      "metadata": {
        "id": "m5DQiW2690V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert normalized spectra to PyTorch tensors\n",
        "\n",
        "import torch\n",
        "\n",
        "# Convert to float32 tensor\n",
        "X = torch.tensor(normalized_fluxes, dtype=torch.float32)\n",
        "\n",
        "print(\"Tensor shape:\", X.shape)\n",
        "print(\"Tensor dtype:\", X.dtype)\n"
      ],
      "metadata": {
        "id": "SXMYmgLj992H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a spectral autoencoder\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SpectralAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim=2000, latent_dim=32):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, latent_dim)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        x_recon = self.decoder(z)\n",
        "        return x_recon, z\n",
        "\n",
        "# Instantiate model\n",
        "model = SpectralAutoencoder(input_dim=X.shape[1], latent_dim=32)\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "DzhuEMN0-N-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the autoencoder\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "dataset = TensorDataset(X)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "EPOCHS = 30\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        batch_x = batch[0]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        recon, _ = model(batch_x)\n",
        "        loss = criterion(recon, batch_x)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - Reconstruction Loss: {avg_loss:.6f}\")\n"
      ],
      "metadata": {
        "id": "pkdttMf_-Zv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract latent embeddings\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, latent_vectors = model(X)\n",
        "\n",
        "latent_vectors = latent_vectors.numpy()\n",
        "\n",
        "print(\"Latent embedding shape:\", latent_vectors.shape)\n"
      ],
      "metadata": {
        "id": "eN6Fh-yO-lI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Anomaly detection using Isolation Forest\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Initialize model\n",
        "iso_forest = IsolationForest(\n",
        "    n_estimators=100,\n",
        "    contamination=0.2,   # expect ~20% anomalies (adjustable)\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit on latent space\n",
        "iso_forest.fit(latent_vectors)\n",
        "\n",
        "# Predict anomalies\n",
        "anomaly_labels = iso_forest.predict(latent_vectors)\n",
        "anomaly_scores = iso_forest.decision_function(latent_vectors)\n",
        "\n",
        "# Convert labels: -1 = anomaly, +1 = normal\n",
        "results = pd.DataFrame({\n",
        "    'spectrum_index': df_stars['index'],\n",
        "    'anomaly_label': anomaly_labels,\n",
        "    'anomaly_score': anomaly_scores\n",
        "})\n",
        "\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "ZRJg8Z7u-5UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize latent space with PCA\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Reduce latent vectors to 2D\n",
        "pca = PCA(n_components=2)\n",
        "latent_2d = pca.fit_transform(latent_vectors)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6, 5))\n",
        "\n",
        "for i in range(len(latent_2d)):\n",
        "    if anomaly_labels[i] == -1:\n",
        "        plt.scatter(latent_2d[i, 0], latent_2d[i, 1],\n",
        "                    color='red', s=100, label='Anomaly' if i == 0 else \"\")\n",
        "    else:\n",
        "        plt.scatter(latent_2d[i, 0], latent_2d[i, 1],\n",
        "                    color='blue', s=80, label='Normal' if i == 0 else \"\")\n",
        "\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.title(\"Latent Space: Normal vs Anomalous Spectra\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ce4WPDDf_KGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot anomalous vs normal spectra\n",
        "\n",
        "# Identify indices\n",
        "anomaly_idx = np.where(anomaly_labels == -1)[0][0]\n",
        "normal_idx = np.where(anomaly_labels == 1)[0][0]\n",
        "\n",
        "anomaly_spec = df_stars.iloc[anomaly_idx]\n",
        "normal_spec = df_stars.iloc[normal_idx]\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.plot(anomaly_spec['wavelength'], anomaly_spec['flux'],\n",
        "         label='Anomalous Spectrum', color='red', lw=1)\n",
        "\n",
        "plt.plot(normal_spec['wavelength'], normal_spec['flux'],\n",
        "         label='Normal Spectrum', color='blue', lw=1, alpha=0.7)\n",
        "\n",
        "plt.xlabel(\"Wavelength (Å)\")\n",
        "plt.ylabel(\"Flux\")\n",
        "plt.title(\"Comparison: Anomalous vs Normal Stellar Spectrum\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Gux3hao0_i5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reconstruction error per spectrum\n",
        "\n",
        "model.eval()\n",
        "\n",
        "reconstruction_errors = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    reconstructions, _ = model(X)\n",
        "\n",
        "    for i in range(X.shape[0]):\n",
        "        original = X[i]\n",
        "        reconstructed = reconstructions[i]\n",
        "\n",
        "        # Mean Squared Error per spectrum\n",
        "        mse = torch.mean((original - reconstructed) ** 2).item()\n",
        "        reconstruction_errors.append(mse)\n",
        "\n",
        "reconstruction_errors = np.array(reconstruction_errors)\n",
        "\n",
        "# Create results table\n",
        "recon_results = pd.DataFrame({\n",
        "    'spectrum_index': df_stars['index'],\n",
        "    'reconstruction_error': reconstruction_errors\n",
        "})\n",
        "\n",
        "# Sort by error (highest = most anomalous)\n",
        "recon_results = recon_results.sort_values(\n",
        "    by='reconstruction_error',\n",
        "    ascending=False\n",
        ")\n",
        "\n",
        "print(recon_results)\n"
      ],
      "metadata": {
        "id": "jpA2MAti_9I4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine anomaly scores\n",
        "\n",
        "# Normalize scores\n",
        "iso_scores_norm = (anomaly_scores - anomaly_scores.min()) / (anomaly_scores.max() - anomaly_scores.min())\n",
        "recon_scores_norm = (reconstruction_errors - reconstruction_errors.min()) / (reconstruction_errors.max() - reconstruction_errors.min())\n",
        "\n",
        "combined_score = iso_scores_norm + recon_scores_norm\n",
        "\n",
        "combined_results = pd.DataFrame({\n",
        "    'spectrum_index': df_stars['index'],\n",
        "    'iso_score': iso_scores_norm,\n",
        "    'recon_score': recon_scores_norm,\n",
        "    'combined_score': combined_score\n",
        "}).sort_values(by='combined_score', ascending=False)\n",
        "\n",
        "print(combined_results)\n"
      ],
      "metadata": {
        "id": "wQ4ITr6PAN6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Real-time inference setup\n",
        "\n",
        "model.eval()\n",
        "\n",
        "def process_single_spectrum(spectrum_tensor):\n",
        "    \"\"\"\n",
        "    Simulates real-time processing of ONE spectrum.\n",
        "    Returns:\n",
        "        reconstruction_error\n",
        "        isolation_forest_score\n",
        "        combined_score\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        recon, latent = model(spectrum_tensor.unsqueeze(0))\n",
        "\n",
        "        # Reconstruction error\n",
        "        recon_error = torch.mean((spectrum_tensor - recon.squeeze()) ** 2).item()\n",
        "\n",
        "        # Isolation Forest score (use latent vector)\n",
        "        latent_np = latent.numpy()\n",
        "        iso_score = iso_forest.decision_function(latent_np)[0]\n",
        "\n",
        "        return recon_error, iso_score\n"
      ],
      "metadata": {
        "id": "x7EpOK8OAlrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate real-time streaming detection\n",
        "\n",
        "import time\n",
        "\n",
        "print(\"Starting real-time stellar anomaly monitoring...\\n\")\n",
        "\n",
        "for i in range(X.shape[0]):\n",
        "    spectrum_tensor = X[i]\n",
        "\n",
        "    recon_error, iso_score = process_single_spectrum(spectrum_tensor)\n",
        "\n",
        "    status = \"NORMAL\"\n",
        "    if iso_score < 0:   # Isolation Forest convention\n",
        "        status = \"⚠️ ANOMALY DETECTED\"\n",
        "\n",
        "    print(f\"[Stream] Spectrum {i}\")\n",
        "    print(f\"  Reconstruction error : {recon_error:.6f}\")\n",
        "    print(f\"  Isolation score      : {iso_score:.6f}\")\n",
        "    print(f\"  Status               : {status}\")\n",
        "    print(\"-\" * 45)\n",
        "\n",
        "    # Simulate delay (like live data)\n",
        "    time.sleep(1)\n"
      ],
      "metadata": {
        "id": "KMfym4xGBKbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log real-time anomalies\n",
        "\n",
        "alert_log = []\n",
        "\n",
        "for i in range(X.shape[0]):\n",
        "    spectrum_tensor = X[i]\n",
        "    recon_error, iso_score = process_single_spectrum(spectrum_tensor)\n",
        "\n",
        "    if iso_score < 0:  # anomaly condition\n",
        "        alert_log.append({\n",
        "            'spectrum_index': i,\n",
        "            'reconstruction_error': recon_error,\n",
        "            'isolation_score': iso_score,\n",
        "            'alert': 'ANOMALY'\n",
        "        })\n",
        "\n",
        "alerts_df = pd.DataFrame(alert_log)\n",
        "\n",
        "print(\"Real-time anomaly log:\")\n",
        "print(alerts_df)\n",
        "\n",
        "# Save alerts (simulated alert archive)\n",
        "alerts_df.to_csv(\"stellar_anomaly_alerts.csv\", index=False)\n",
        "print(\"\\nAlerts saved to stellar_anomaly_alerts.csv\")\n"
      ],
      "metadata": {
        "id": "HZLhgIKGBZiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Collect SDSS spectra safely without vstack\n",
        "\n",
        "from astroquery.sdss import SDSS\n",
        "from astropy.coordinates import SkyCoord\n",
        "import astropy.units as u\n",
        "\n",
        "SDSS.clear_cache()\n",
        "\n",
        "base_ra = 2.023\n",
        "base_dec = 14.84\n",
        "\n",
        "offsets = [\n",
        "    (0.0, 0.0),\n",
        "    (0.02, 0.0),\n",
        "    (-0.02, 0.0),\n",
        "    (0.0, 0.02),\n",
        "    (0.0, -0.02)\n",
        "]\n",
        "\n",
        "all_spectra_hdus = []\n",
        "\n",
        "print(\"Querying SDSS using multiple safe pointings...\\n\")\n",
        "\n",
        "for dra, ddec in offsets:\n",
        "    coord = SkyCoord(\n",
        "        ra=(base_ra + dra) * u.degree,\n",
        "        dec=(base_dec + ddec) * u.degree,\n",
        "        frame='icrs'\n",
        "    )\n",
        "\n",
        "    matches = SDSS.query_region(\n",
        "        coord,\n",
        "        radius=3 * u.arcmin,\n",
        "        spectro=True\n",
        "    )\n",
        "\n",
        "    if matches is None or len(matches) == 0:\n",
        "        continue\n",
        "\n",
        "    print(f\"Found {len(matches)} spectra at offset ({dra}, {ddec})\")\n",
        "\n",
        "    # Limit per pointing (safety)\n",
        "    matches = matches[:10]\n",
        "\n",
        "    spectra = SDSS.get_spectra(matches=matches)\n",
        "\n",
        "    for hdu in spectra:\n",
        "        all_spectra_hdus.append(hdu)\n",
        "\n",
        "print(f\"\\nTotal spectra collected: {len(all_spectra_hdus)}\")\n"
      ],
      "metadata": {
        "id": "JwSKMBRiBi7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the larger SDSS dataset\n",
        "\n",
        "from scipy.interpolate import interp1d\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Extract wavelength & flux from collected spectra\n",
        "data_list_big = []\n",
        "\n",
        "for i, hdu_list in enumerate(all_spectra_hdus):\n",
        "    try:\n",
        "        header = hdu_list[0].header\n",
        "        data = hdu_list[1].data\n",
        "\n",
        "        wavelength = 10 ** data['loglam']\n",
        "        flux = data['flux']\n",
        "\n",
        "        data_list_big.append({\n",
        "            'index': i,\n",
        "            'wavelength': wavelength,\n",
        "            'flux': flux\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Skipped spectrum {i}: {e}\")\n",
        "\n",
        "df_big = pd.DataFrame(data_list_big)\n",
        "print(f\"Spectra loaded for preprocessing: {len(df_big)}\")\n",
        "\n",
        "#Define common wavelength grid (reuse earlier logic)\n",
        "min_waves = [row['wavelength'].min() for _, row in df_big.iterrows()]\n",
        "max_waves = [row['wavelength'].max() for _, row in df_big.iterrows()]\n",
        "\n",
        "global_min = np.max(min_waves)\n",
        "global_max = np.min(max_waves)\n",
        "\n",
        "NUM_POINTS = 2000\n",
        "common_wavelength_big = np.linspace(global_min, global_max, NUM_POINTS)\n",
        "\n",
        "print(f\"Common wavelength range: {global_min:.1f} Å – {global_max:.1f} Å\")\n",
        "\n",
        "#  Interpolate spectra\n",
        "interpolated_fluxes_big = []\n",
        "\n",
        "for _, row in df_big.iterrows():\n",
        "    interp_func = interp1d(\n",
        "        row['wavelength'],\n",
        "        row['flux'],\n",
        "        bounds_error=False,\n",
        "        fill_value=\"extrapolate\"\n",
        "    )\n",
        "    interpolated_fluxes_big.append(interp_func(common_wavelength_big))\n",
        "\n",
        "interpolated_fluxes_big = np.array(interpolated_fluxes_big)\n",
        "\n",
        "print(\"Interpolated shape:\", interpolated_fluxes_big.shape)\n",
        "\n",
        "#  Normalize spectra\n",
        "normalized_fluxes_big = []\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "for i in range(interpolated_fluxes_big.shape[0]):\n",
        "    spec = interpolated_fluxes_big[i].reshape(-1, 1)\n",
        "    norm_spec = scaler.fit_transform(spec).flatten()\n",
        "    normalized_fluxes_big.append(norm_spec)\n",
        "\n",
        "normalized_fluxes_big = np.array(normalized_fluxes_big)\n",
        "\n",
        "print(\"Normalized shape:\", normalized_fluxes_big.shape)\n",
        "\n",
        "#Convert to PyTorch tensor\n",
        "X_big = torch.tensor(normalized_fluxes_big, dtype=torch.float32)\n",
        "\n",
        "print(\"Final tensor shape:\", X_big.shape)\n"
      ],
      "metadata": {
        "id": "8GHsfRhbCf7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrain autoencoder on larger dataset\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Reinitialize model (fresh training)\n",
        "model_big = SpectralAutoencoder(input_dim=2000, latent_dim=32)\n",
        "\n",
        "# Dataset & loader\n",
        "dataset_big = TensorDataset(X_big)\n",
        "dataloader_big = DataLoader(dataset_big, batch_size=4, shuffle=True)\n",
        "\n",
        "# Loss & optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model_big.parameters(), lr=1e-3)\n",
        "\n",
        "# Training\n",
        "EPOCHS = 50\n",
        "model_big.train()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for batch in dataloader_big:\n",
        "        batch_x = batch[0]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        recon, _ = model_big(batch_x)\n",
        "        loss = criterion(recon, batch_x)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(dataloader_big)\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - Reconstruction Loss: {avg_loss:.6f}\")\n"
      ],
      "metadata": {
        "id": "80_LHVJiC08y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Extract latent embeddings\n",
        "model_big.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, latent_big = model_big(X_big)\n",
        "\n",
        "latent_big = latent_big.numpy()\n",
        "print(\"Latent shape:\", latent_big.shape)\n",
        "\n",
        "#  Isolation Forest\n",
        "iso_forest_big = IsolationForest(\n",
        "    n_estimators=200,\n",
        "    contamination=0.15,   # assume ~15% anomalies\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "iso_forest_big.fit(latent_big)\n",
        "\n",
        "anomaly_labels_big = iso_forest_big.predict(latent_big)\n",
        "anomaly_scores_big = iso_forest_big.decision_function(latent_big)\n",
        "\n",
        "#  Reconstruction error\n",
        "reconstruction_errors_big = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    recon_big, _ = model_big(X_big)\n",
        "\n",
        "    for i in range(X_big.shape[0]):\n",
        "        mse = torch.mean((X_big[i] - recon_big[i]) ** 2).item()\n",
        "        reconstruction_errors_big.append(mse)\n",
        "\n",
        "reconstruction_errors_big = np.array(reconstruction_errors_big)\n",
        "\n",
        "#  Normalize & combine scores\n",
        "iso_norm = (anomaly_scores_big - anomaly_scores_big.min()) / (anomaly_scores_big.max() - anomaly_scores_big.min())\n",
        "recon_norm = (reconstruction_errors_big - reconstruction_errors_big.min()) / (reconstruction_errors_big.max() - reconstruction_errors_big.min())\n",
        "\n",
        "combined_score_big = iso_norm + recon_norm\n",
        "\n",
        "# Results table\n",
        "results_big = pd.DataFrame({\n",
        "    'spectrum_index': np.arange(len(X_big)),\n",
        "    'iso_score': iso_norm,\n",
        "    'recon_error': recon_norm,\n",
        "    'combined_score': combined_score_big,\n",
        "    'label': anomaly_labels_big\n",
        "}).sort_values(by='combined_score', ascending=False)\n",
        "\n",
        "print(results_big)\n"
      ],
      "metadata": {
        "id": "wgsIATKBDd6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model_big.eval()\n",
        "\n",
        "for param in model_big.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "print(\"Model frozen for inference.\")\n"
      ],
      "metadata": {
        "id": "U1PlQAQmE5q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def streaming_inference(\n",
        "    spectrum_tensor,\n",
        "    model,\n",
        "    iso_model\n",
        "):\n",
        "    \"\"\"\n",
        "    Process ONE incoming spectrum in real time.\n",
        "    Returns a dictionary with anomaly information.\n",
        "    \"\"\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Forward pass\n",
        "        recon, latent = model(spectrum_tensor.unsqueeze(0))\n",
        "\n",
        "        # Reconstruction error\n",
        "        recon_error = torch.mean((spectrum_tensor - recon.squeeze()) ** 2).item()\n",
        "\n",
        "        # Isolation Forest score\n",
        "        latent_np = latent.numpy()\n",
        "        iso_score = iso_model.decision_function(latent_np)[0]\n",
        "        iso_label = iso_model.predict(latent_np)[0]\n",
        "\n",
        "        # Combined score (simple fusion)\n",
        "        combined_score = recon_error - iso_score  # higher = more anomalous\n",
        "\n",
        "    return {\n",
        "        \"reconstruction_error\": recon_error,\n",
        "        \"isolation_score\": iso_score,\n",
        "        \"isolation_label\": iso_label,\n",
        "        \"combined_score\": combined_score\n",
        "    }\n",
        "\n",
        "print(\"Streaming inference engine ready.\")\n"
      ],
      "metadata": {
        "id": "SFGX44SyFVLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import time\n",
        "\n",
        "candidate_log = []\n",
        "\n",
        "print(\"Starting continuous stellar spectrum stream...\\n\")\n",
        "\n",
        "for i in range(len(X_big)):\n",
        "    spectrum = X_big[i]\n",
        "\n",
        "    result = streaming_inference(\n",
        "        spectrum_tensor=spectrum,\n",
        "        model=model_big,\n",
        "        iso_model=iso_forest_big\n",
        "    )\n",
        "\n",
        "    print(f\"[STREAM] Spectrum {i}\")\n",
        "    print(f\"  Reconstruction error : {result['reconstruction_error']:.6f}\")\n",
        "    print(f\"  Isolation score      : {result['isolation_score']:.6f}\")\n",
        "    print(f\"  Combined score       : {result['combined_score']:.6f}\")\n",
        "\n",
        "    # Decide anomaly\n",
        "    if result[\"isolation_label\"] == -1:\n",
        "        print(\"  ⚠️  FLAGGED AS ANOMALY\\n\")\n",
        "\n",
        "        candidate_log.append({\n",
        "            \"spectrum_index\": i,\n",
        "            \"reconstruction_error\": result[\"reconstruction_error\"],\n",
        "            \"isolation_score\": result[\"isolation_score\"],\n",
        "            \"combined_score\": result[\"combined_score\"]\n",
        "        })\n",
        "    else:\n",
        "        print(\"  Status: Normal\\n\")\n",
        "\n",
        "    time.sleep(0.5)  # simulate streaming delay\n",
        "\n",
        "print(\"Streaming complete.\")\n",
        "print(f\"Total candidates flagged: {len(candidate_log)}\")\n"
      ],
      "metadata": {
        "id": "kdigJ3wNFeV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Long-running batch ingestion simulation\n",
        "\n",
        "import time\n",
        "\n",
        "long_run_candidates = []\n",
        "\n",
        "NUM_CYCLES = 5   # simulate 5 observing cycles\n",
        "\n",
        "print(\"Starting long-running observation simulation...\\n\")\n",
        "\n",
        "for cycle in range(NUM_CYCLES):\n",
        "    print(f\"=== Observation cycle {cycle+1}/{NUM_CYCLES} ===\")\n",
        "\n",
        "    for i in range(len(X_big)):\n",
        "        spectrum = X_big[i]\n",
        "\n",
        "        result = streaming_inference(\n",
        "            spectrum_tensor=spectrum,\n",
        "            model=model_big,\n",
        "            iso_model=iso_forest_big\n",
        "        )\n",
        "\n",
        "        if result[\"isolation_label\"] == -1:\n",
        "            long_run_candidates.append({\n",
        "                \"cycle\": cycle,\n",
        "                \"spectrum_index\": i,\n",
        "                \"reconstruction_error\": result[\"reconstruction_error\"],\n",
        "                \"isolation_score\": result[\"isolation_score\"],\n",
        "                \"combined_score\": result[\"combined_score\"]\n",
        "            })\n",
        "\n",
        "    time.sleep(0.5)\n",
        "\n",
        "print(\"\\nLong-running ingestion complete.\")\n",
        "print(f\"Total anomaly events detected: {len(long_run_candidates)}\")\n"
      ],
      "metadata": {
        "id": "q2g8jJiZFyBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify stable anomaly candidates\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Convert to DataFrame\n",
        "long_run_df = pd.DataFrame(long_run_candidates)\n",
        "\n",
        "# Count how many times each spectrum was flagged\n",
        "candidate_summary = (\n",
        "    long_run_df\n",
        "    .groupby(\"spectrum_index\")\n",
        "    .size()\n",
        "    .reset_index(name=\"times_flagged\")\n",
        "    .sort_values(by=\"times_flagged\", ascending=False)\n",
        ")\n",
        "\n",
        "print(\"Stable anomaly candidates:\")\n",
        "print(candidate_summary)\n"
      ],
      "metadata": {
        "id": "QR1gMwOEGACU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect stable anomaly spectra (raw view)\n",
        "\n",
        "candidate_indices = [3, 12]\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "for idx in candidate_indices:\n",
        "    row = df_big.iloc[idx]\n",
        "    plt.plot(\n",
        "        row['wavelength'],\n",
        "        row['flux'],\n",
        "        label=f\"Candidate spectrum {idx}\",\n",
        "        lw=1\n",
        "    )\n",
        "\n",
        "plt.xlabel(\"Wavelength (Å)\")\n",
        "plt.ylabel(\"Flux\")\n",
        "plt.title(\"Stable Anomaly Candidate Spectra\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LwuoFkc5GNOF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}